rag-backup-assistant/
├── data/                  # Fichiers à indexer (tu les ajoutes ici)
│   ├── logs/
│   ├── images/
│   └── policies/
├── db/                    # Persisted Chroma DB
├── ingest.py
├── chat.py
├── config.yaml            # Pour options futures (langue, modèle, etc.)
├── requirements.txt
└── README.md


curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral


🧪 Étapes d'exécution
Installe les dépendances :
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

Lance l’indexation :
python ingest.py

Ouvre le chat :
python chat.py


Chroma require sqlite, use faiss-cpu
pip install faiss-cpu


LLAMA CPP : 

pip install llama-cpp-python

✅ 2. Adaptation dans chat.py

Remplace :

from llm_wrapper import CTransformersLLM
...
llm = CTransformersLLM(...)
par :

from llm_wrapper_cpp import LlamaCppLLM
...
llm = LlamaCppLLM(
    model_path="models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=4,
    n_batch=8,
    temperature=0.1,
    max_tokens=512,
)





import json

# Mapping utile des jours (selon NetBackup, 1 = Monday... 7 = Sunday)
DAYS_MAP = {
    1: "Monday", 2: "Tuesday", 3: "Wednesday",
    4: "Thursday", 5: "Friday", 6: "Saturday", 7: "Sunday"
}

def parse_recurring_days(recs, is_week=True):
    """Convert '1:6' to 'Monday at 6h' if is_week, else just 'Day X'."""
    parsed = []
    for r in recs:
        if is_week and ':' in r:
            day, hour = r.split(':')
            parsed.append(f"{DAYS_MAP.get(int(day), 'Day '+day)} at {hour}h")
        else:
            parsed.append(r)
    return parsed

def simplify_schedules(schedules):
    simplified = []
    summaries = []

    for sched in schedules:
        name = sched.get("scheduleName", "<unnamed>")
        btype = sched.get("backupType", "Unknown")
        freq_days = sched.get("frequencySeconds", 0) // 86400 if sched.get("frequencySeconds") else None

        # Retention & Storage
        copies = sched.get("backupCopies", {}).get("copies", [])
        if copies:
            cp = copies[0]
            retention = f"{cp['retentionPeriod']['value']} {cp['retentionPeriod']['unit']}"
            storage = cp.get("storage", "Unknown")
        else:
            retention = storage = None

        # Windows
        windows = []
        for w in sched.get("startWindow", []):
            dur_h = w["durationSeconds"] // 3600
            if dur_h > 0:
                windows.append({
                    "dayName": DAYS_MAP.get(w["dayOfWeek"], f"Day {w['dayOfWeek']}"),
                    "startHour": w["startSeconds"] // 3600,
                    "durationHours": dur_h
                })

        # Include Dates (recurring week/month)
        inc = sched.get("includeDates", {})
        inc_days = parse_recurring_days(inc.get("recurringDaysOfWeek", []), is_week=True)
        inc_month = parse_recurring_days(inc.get("recurringDaysOfMonth", []), is_week=False)
        inc_spec = inc.get("specificDates", [])
        includes = inc_days + inc_month + inc_spec

        # Exclude Dates
        excl = sched.get("excludeDates", {})
        excludes = []
        if excl.get("lastDayOfMonth"):
            excludes.append("Last day of month")
        excludes += parse_recurring_days(excl.get("recurringDaysOfWeek", []), is_week=True)
        excludes += parse_recurring_days(excl.get("recurringDaysOfMonth", []), is_week=False)
        excludes += excl.get("specificDates", [])

        simplified.append({
            "name": name,
            "type": btype,
            "frequency_days": freq_days,
            "retention": retention,
            "storage": storage,
            "windows": windows,
            "includes": includes,
            "excludes": excludes,
            "acceleratorForcedRescan": sched.get("acceleratorForcedRescan", False),
            "mediaMultiplexing": sched.get("mediaMultiplexing"),
            "retriesAllowedAfterRunDay": sched.get("retriesAllowedAfterRunDay"),
            "syntheticBackup": sched.get("syntheticBackup"),
        })

        # Build summary
        win_desc = "; ".join(f"{w['dayName']} at {w['startHour']}h for {w['durationHours']}h"
                             for w in windows) or "no active window"
        inc_desc = f" Includes: {', '.join(includes)}." if includes else ""
        excl_desc = f" Excludes: {', '.join(excludes)}." if excludes else ""
        acc_desc = " with accelerator forced rescan" if sched.get("acceleratorForcedRescan") else ""
        retry_desc = " retries allowed on next day." if sched.get("retriesAllowedAfterRunDay") else ""
        synth_desc = " (synthetic)" if sched.get("syntheticBackup") else ""
        mux_desc = f" Multiplexing: {sched.get('mediaMultiplexing')}." if sched.get("mediaMultiplexing") else ""

        summary = (
            f"Schedule '{name}' is a {btype.lower()}{synth_desc} every {freq_days} days, "
            f"stored on {storage} with retention {retention}{acc_desc}.{mux_desc}"
            f" Runs: {win_desc}.{inc_desc}{excl_desc}{retry_desc}"
        )
        summaries.append(summary)

    return {"simplified": simplified, "summaries": summaries}


# Usage example
schedules = [ ... ]  # ton input ici
result = simplify_schedules(schedules)
print(json.dumps(result, indent=2))



import json
print(json.dumps(simplify_schedules(schedule_json), indent=2))





--------------------



import requests
from datetime import datetime

# Liste de masters NetBackup
MASTERS = [
    {"url": "https://master1.domain.com:1556/netbackup", "user": "admin", "password": "pass"},
    {"url": "https://master2.domain.com:1556/netbackup", "user": "admin", "password": "pass"},
]

def login(master):
    url = f"{master['url']}/login"
    r = requests.post(url, json={"userName": master["user"], "password": master["password"]}, verify=False)
    r.raise_for_status()
    return r.json()["token"]

def search_files(master, token, query, start_date=None, end_date=None):
    url = f"{master['url']}/catalog/images/files"

    params = {
        "pageSize": 100,
        "filePath": f"*{query}*"
    }
    if start_date:
        params["startTime"] = start_date.strftime("%Y-%m-%dT%H:%M:%SZ")
    if end_date:
        params["endTime"] = end_date.strftime("%Y-%m-%dT%H:%M:%SZ")

    headers = {"Authorization": f"Bearer {token}"}

    results = []
    while url:
        r = requests.get(url, headers=headers, params=params, verify=False)
        r.raise_for_status()
        data = r.json()

        for f in data.get("files", []):
            results.append({
                "fileName": f.get("fileName"),
                "path": f.get("path"),
                "backupTime": f.get("backupTime"),
                "client": f.get("clientName"),
                "policy": f.get("policyName"),
                "storage": f.get("storageUnit"),
                "master": master["url"]
            })

        url = data.get("nextPage")

    return results


def global_search(query, start_date=None, end_date=None):
    all_results = []
    for master in MASTERS:
        try:
            token = login(master)
            results = search_files(master, token, query, start_date, end_date)
            all_results.extend(results)
        except Exception as e:
            print(f"Error with {master['url']}: {e}")
    return all_results


if __name__ == "__main__":
    # Exemple : chercher "toto" entre 2023-01-01 et 2023-12-31
    start = datetime(2023, 1, 1)
    end = datetime(2023, 12, 31)
    results = global_search("toto", start, end)

    for r in results:
        print(f"[{r['master']}] {r['backupTime']} - {r['client']} - {r['path']}/{r['fileName']} (Policy={r['policy']}, Storage={r['storage']})")





-----------------------




input {
  kafka {
    bootstrap_servers => "kafka1:9092,kafka2:9092"
    topics => ["netbackup-logs"]
    group_id => "logstash-netbackup"
    codec => "json"
  }
}

filter {
  # On s'assure que @tenant et @type existent
  if ![ @tenant ] {
    mutate { add_field => { "[@tenant]" => "default-tenant" } }
  }
  if ![ @type ] {
    mutate { add_field => { "[@type]" => "unknown" } }
  }

  # Création d'un champ index_name dynamique
  mutate {
    add_field => {
      "[@metadata][index_name]" => "%{[@tenant]}-%{[@type]}"
    }
  }

  # Exemple: ajouter des champs supplémentaires selon le type
  if [@type] == "backup" {
    mutate { add_field => { "category" => "backup-files" } }
  } else if [@type] == "job" {
    mutate { add_field => { "category" => "backup-jobs" } }
  } else if [@type] == "image" {
    mutate { add_field => { "category" => "backup-images" } }
  }
}

output {
  elasticsearch {
    hosts => ["https://es-cluster:9200"]
    user => "logstash_user"
    password => "secret"
    index => "%{[@metadata][index_name]}-%{+YYYY.MM.dd}"
    ssl => true
    cacert => "/etc/logstash/certs/ca.crt"
  }

  # Optionnel: log local pour debug
  stdout { codec => rubydebug }
}



from confluent_kafka import Producer
import json
from datetime import datetime

KAFKA_BOOTSTRAP = "kafka1:9092,kafka2:9092"
TOPIC = "netbackup-logs"

producer = Producer({'bootstrap.servers': KAFKA_BOOTSTRAP})

def send_log(data: dict):
    """
    Envoie un log JSON vers Kafka.
    data doit contenir au minimum @tenant et @type
    """
    if "@tenant" not in data:
        data["@tenant"] = "default-tenant"
    if "@type" not in data:
        data["@type"] = "unknown"

    # Ajouter timestamp UTC si pas présent
    if "@timestamp" not in data:
        data["@timestamp"] = datetime.utcnow().isoformat()

    producer.produce(
        topic=TOPIC,
        value=json.dumps(data).encode("utf-8"),
        callback=lambda err, msg: print("✅ Sent" if err is None else f"❌ Failed {err}")
    )
    producer.flush()

# Exemple
if __name__ == "__main__":
    events = [
        {"@tenant": "customerA", "@type": "backup", "client": "srv01", "file": "/drive/folder/toto.txt"},
        {"@tenant": "customerA", "@type": "job", "jobId": 1234, "status": "COMPLETED"},
        {"@tenant": "customerA", "@type": "image", "imageId": 5678, "sizeMB": 1024},
    ]

    for e in events:
        send_log(e)

