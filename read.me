rag-backup-assistant/
â”œâ”€â”€ data/                  # Fichiers Ã  indexer (tu les ajoutes ici)
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ policies/
â”œâ”€â”€ db/                    # Persisted Chroma DB
â”œâ”€â”€ ingest.py
â”œâ”€â”€ chat.py
â”œâ”€â”€ config.yaml            # Pour options futures (langue, modÃ¨le, etc.)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral


ðŸ§ª Ã‰tapes d'exÃ©cution
Installe les dÃ©pendances :
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

Lance lâ€™indexation :
python ingest.py

Ouvre le chat :
python chat.py


Chroma require sqlite, use faiss-cpu
pip install faiss-cpu


LLAMA CPP : 

pip install llama-cpp-python

âœ… 2. Adaptation dans chat.py

Remplace :

from llm_wrapper import CTransformersLLM
...
llm = CTransformersLLM(...)
par :

from llm_wrapper_cpp import LlamaCppLLM
...
llm = LlamaCppLLM(
    model_path="models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=4,
    n_batch=8,
    temperature=0.1,
    max_tokens=512,
)
